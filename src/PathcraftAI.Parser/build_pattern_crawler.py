"""
Build Pattern Crawler - ë¹Œë“œ ì „í™˜ íŒ¨í„´ ìˆ˜ì§‘ê¸°

Reddit, ë””ì‹œì¸ì‚¬ì´ë“œ, GitHubì—ì„œ ë ˆë²¨ë§ â†’ ìµœì¢… ë¹Œë“œ ì „í™˜ íŒ¨í„´ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
"""

import json
import re
import time
import base64
import sys
import io
from datetime import datetime
from pathlib import Path
from typing import Optional
import requests
from bs4 import BeautifulSoup

# Fix encoding for Windows console
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

# Constants
DATA_DIR = Path(__file__).parent / "data"
OUTPUT_FILE = DATA_DIR / "build_transition_patterns.json"

# Rate limits
REDDIT_DELAY = 1.0  # 1 request per second (60/min limit)
DCINSIDE_DELAY = 0.5
GITHUB_DELAY = 2.0  # More conservative for GitHub

# POB code pattern
POB_PATTERN = re.compile(r'(?:pastebin\.com/\w+|poe\.?b(?:in)?\.party/[\w-]+|[A-Za-z0-9+/=]{50,})')

# Common leveling skills mapping (expanded)
LEVELING_SKILLS = {
    # Brands
    "armageddon brand": ["penance brand", "storm brand", "wintertide brand", "penance brand of dissipation"],
    "storm brand": ["penance brand", "arc", "spark", "penance brand of dissipation"],
    "stormblast mine": ["icicle mine", "pyroclast mine", "arc", "ball lightning"],
    "wintertide brand": ["vortex", "cold snap"],

    # Spells - Fire
    "rolling magma": ["fireball", "flame surge", "blazing salvo", "detonate dead"],
    "flame wall": ["fireball", "righteous fire"],
    "arcanist brand": ["blazing salvo", "fireball"],
    "cremation": ["detonate dead"],

    # Spells - Cold
    "freezing pulse": ["ice nova", "cold snap", "vortex", "vaal ice nova"],
    "frostbolt": ["ice nova", "vortex"],
    "ice spear": ["ice nova"],

    # Spells - Lightning
    "arc": ["storm brand", "spark", "ball lightning", "vaal spark"],
    "spark": ["vaal spark", "arc", "ball lightning"],
    "orb of storms": ["arc", "spark", "ball lightning"],
    "storm call": ["arc"],

    # Spells - Chaos/Phys
    "essence drain": ["bane", "soulrend"],
    "blight": ["bane", "essence drain"],

    # Totems
    "holy flame totem": ["righteous fire", "flame surge", "cremation"],
    "freezing pulse totem": ["ice nova", "glacial cascade"],

    # Attacks - Melee
    "splitting steel": ["lancing steel", "shattering steel", "spectral throw"],
    "cleave": ["cyclone", "lacerate", "bladestorm"],
    "ground slam": ["earthquake", "tectonic slam", "ice crash"],
    "sunder": ["earthquake", "cyclone"],
    "sweep": ["cyclone", "bladestorm"],
    "perforate": ["lacerate", "bladestorm"],
    "double strike": ["blade flurry", "reave"],

    # Attacks - Ranged
    "spectral helix": ["lightning strike", "frost blades", "molten strike"],
    "rain of arrows": ["tornado shot", "lightning arrow", "ice shot"],
    "burning arrow": ["tornado shot", "lightning arrow"],
    "galvanic arrow": ["lightning arrow", "tornado shot"],
    "caustic arrow": ["toxic rain", "scourge arrow"],

    # Minions
    "summon raging spirit": ["summon skeletons", "raise zombie", "raise spectre"],
    "absolution": ["dominating blow", "herald of purity"],
    "summon holy relic": ["dominating blow"],
    "animate weapon": ["summon skeletons"],
}

# Final skills that typically need leveling alternatives (expanded)
FINAL_BUILD_SKILLS = [
    # Brands
    "penance brand", "penance brand of dissipation", "storm brand", "wintertide brand",
    # Fire
    "righteous fire", "fireball", "blazing salvo", "flame surge", "detonate dead",
    # Cold
    "vortex", "cold snap", "ice nova", "vaal ice nova", "glacial cascade",
    # Lightning
    "spark", "vaal spark", "arc", "ball lightning", "storm call",
    # Chaos
    "bane", "essence drain", "soulrend",
    # Bow
    "tornado shot", "lightning arrow", "ice shot", "toxic rain", "scourge arrow",
    # Melee
    "cyclone", "lacerate", "bladestorm", "earthquake", "tectonic slam", "ice crash",
    "blade flurry", "reave", "lightning strike", "frost blades", "molten strike",
    # Minions
    "summon skeletons", "raise spectre", "raise zombie", "dominating blow", "herald of purity",
    # Totems/Traps/Mines
    "icicle mine", "pyroclast mine",
]


class BuildPatternCrawler:
    """ë¹Œë“œ ì „í™˜ íŒ¨í„´ ìˆ˜ì§‘ê¸°"""

    def __init__(self):
        self.patterns = []
        self.stats = {
            "reddit": {"posts_scanned": 0, "patterns_found": 0},
            "dcinside": {"posts_scanned": 0, "patterns_found": 0},
            "github": {"repos_scanned": 0, "patterns_found": 0},
        }
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "PathcraftAI/1.0 (Build Pattern Collector)"
        })

    def save_patterns(self):
        """ìˆ˜ì§‘ëœ íŒ¨í„´ì„ JSONìœ¼ë¡œ ì €ì¥"""
        # Filter out invalid patterns
        valid_patterns = []
        invalid_words = ["on popular", "guide", "build", "video", "index", "link"]

        for pattern in self.patterns:
            leveling = pattern.get("leveling_skill", "").lower()
            final = pattern.get("final_skill", "").lower()

            # Skip if leveling skill contains invalid words
            if any(word in leveling for word in invalid_words):
                continue
            # Skip if skills are the same
            if leveling == final:
                continue
            # Skip if skill name is too short
            if len(leveling) < 3 or len(final) < 3:
                continue

            valid_patterns.append(pattern)

        # Remove duplicates
        seen = set()
        unique_patterns = []
        for p in valid_patterns:
            key = (p["leveling_skill"].lower(), p["final_skill"].lower())
            if key not in seen:
                seen.add(key)
                unique_patterns.append(p)

        output = {
            "version": "1.0",
            "updated": datetime.now().isoformat(),
            "stats": self.stats,
            "total_patterns": len(unique_patterns),
            "patterns": unique_patterns
        }

        DATA_DIR.mkdir(parents=True, exist_ok=True)
        with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
            json.dump(output, f, indent=2, ensure_ascii=False)

        print(f"\nâœ… ì €ì¥ ì™„ë£Œ: {OUTPUT_FILE}")
        print(f"   ì´ {len(self.patterns)}ê°œ íŒ¨í„´ ìˆ˜ì§‘")

    def extract_skills_from_text(self, text: str) -> list:
        """í…ìŠ¤íŠ¸ì—ì„œ ìŠ¤í‚¬ ì´ë¦„ ì¶”ì¶œ"""
        text_lower = text.lower()
        found_skills = []

        # Valid skill names for validation
        all_valid_skills = set()
        for leveling in LEVELING_SKILLS.keys():
            all_valid_skills.add(leveling)
        for finals in LEVELING_SKILLS.values():
            for final in finals:
                all_valid_skills.add(final)
        for skill in FINAL_BUILD_SKILLS:
            all_valid_skills.add(skill)

        # Check for leveling skills
        for leveling, finals in LEVELING_SKILLS.items():
            if leveling in text_lower:
                for final in finals:
                    if final in text_lower:
                        found_skills.append({
                            "leveling_skill": leveling.title(),
                            "final_skill": final.title()
                        })

        # Check for explicit leveling patterns with known skills only
        leveling_patterns = [
            r"level(?:ing)?\s+(?:with|as|using)\s+([\w\s]+?)(?:\s+(?:until|then|before|->))",
            r"([\w\s]+?)\s+(?:for|during|in)\s+(?:acts?|leveling)",
            r"(?:acts?|leveling).*?:\s*([\w\s]+?)(?:\s*(?:->|â†’|to)\s*)([\w\s]+)",
        ]

        for pattern in leveling_patterns:
            matches = re.findall(pattern, text_lower)
            for match in matches:
                if isinstance(match, tuple):
                    leveling_candidate = match[0].strip()
                    final_candidate = match[1].strip() if len(match) > 1 else None
                else:
                    leveling_candidate = match.strip()
                    final_candidate = None

                # Only add if leveling skill is in valid skills list
                if leveling_candidate in all_valid_skills:
                    # Find what final skill this might transition to
                    for final_skill in FINAL_BUILD_SKILLS:
                        if final_skill in text_lower and final_skill != leveling_candidate:
                            found_skills.append({
                                "leveling_skill": leveling_candidate.title(),
                                "final_skill": final_skill.title()
                            })
                            break

        return found_skills

    def extract_pob_code(self, text: str) -> Optional[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ POB ì½”ë“œ ì¶”ì¶œ"""
        match = POB_PATTERN.search(text)
        if match:
            return match.group(0)
        return None

    # ==================== Reddit Crawler ====================

    def crawl_reddit(self, subreddit: str = "PathOfExileBuilds", limit: int = 100):
        """Redditì—ì„œ ë¹Œë“œ ê°€ì´ë“œ ìˆ˜ì§‘ (OAuth ì—†ì´ JSON API ì‚¬ìš©)"""
        print(f"\nğŸ” Reddit r/{subreddit} í¬ë¡¤ë§ ì‹œì‘...")

        base_url = f"https://www.reddit.com/r/{subreddit}"

        # Search queries for build guides with leveling info
        search_queries = [
            "leveling guide",
            "league starter",
            "act to maps",
            "leveling setup",
            "level with",
            "transition to",
            "armageddon brand",
            "penance brand",
            "storm brand",
            "rolling magma",
            "spectral helix",
            "lightning strike",
            "spark leveling",
            "arc leveling",
            "ground slam earthquake",
            "freezing pulse ice nova",
        ]

        posts_processed = set()

        for query in search_queries:
            try:
                # Use Reddit's JSON API
                search_url = f"{base_url}/search.json"
                params = {
                    "q": query,
                    "restrict_sr": "on",
                    "sort": "relevance",
                    "limit": 25,  # Max per query
                    "t": "all"  # All time
                }

                response = self.session.get(search_url, params=params, timeout=10)

                if response.status_code == 429:  # Rate limited
                    print(f"   âš ï¸ Rate limited, waiting...")
                    time.sleep(60)
                    continue

                if response.status_code != 200:
                    print(f"   âš ï¸ Error {response.status_code} for query: {query}")
                    continue

                data = response.json()
                posts = data.get("data", {}).get("children", [])

                for post in posts:
                    post_data = post.get("data", {})
                    post_id = post_data.get("id")

                    if post_id in posts_processed:
                        continue
                    posts_processed.add(post_id)

                    title = post_data.get("title", "")
                    selftext = post_data.get("selftext", "")
                    url = f"https://reddit.com{post_data.get('permalink', '')}"

                    full_text = f"{title}\n{selftext}"

                    # Extract patterns
                    skills = self.extract_skills_from_text(full_text)
                    pob_code = self.extract_pob_code(full_text)

                    for skill_pair in skills:
                        pattern = {
                            "final_skill": skill_pair["final_skill"],
                            "leveling_skill": skill_pair["leveling_skill"],
                            "class": self._extract_class(full_text),
                            "ascendancy": self._extract_ascendancy(full_text),
                            "transition_point": self._guess_transition_point(full_text),
                            "source": "reddit",
                            "url": url,
                            "pob_code": pob_code
                        }
                        self.patterns.append(pattern)
                        self.stats["reddit"]["patterns_found"] += 1

                    self.stats["reddit"]["posts_scanned"] += 1

                time.sleep(REDDIT_DELAY)

            except Exception as e:
                print(f"   âŒ Error: {e}")
                continue

        print(f"   âœ… Reddit: {self.stats['reddit']['posts_scanned']}ê°œ í¬ìŠ¤íŠ¸ ìŠ¤ìº”, "
              f"{self.stats['reddit']['patterns_found']}ê°œ íŒ¨í„´ ë°œê²¬")

    # ==================== DCInside Crawler ====================

    def crawl_dcinside(self, gallery_id: str = "pathofexile", limit: int = 100):
        """ë””ì‹œì¸ì‚¬ì´ë“œ ê°¤ëŸ¬ë¦¬ì—ì„œ ë¹Œë“œ ê°€ì´ë“œ ìˆ˜ì§‘"""
        print(f"\nğŸ” ë””ì‹œì¸ì‚¬ì´ë“œ {gallery_id} ê°¤ëŸ¬ë¦¬ í¬ë¡¤ë§ ì‹œì‘...")

        # POE ë§ˆì´ë„ˆ ê°¤ëŸ¬ë¦¬ URL
        base_url = "https://gall.dcinside.com/mgallery/board/lists"

        # Search keywords
        search_keywords = ["ë ˆë²¨ë§", "ì•¡íŠ¸", "ê°€ì´ë“œ", "ë¹Œë“œ", "ë‚™ì¸", "ì¶”ì²œ"]

        posts_processed = set()
        page = 1
        max_pages = max(1, limit // 20)  # ~20 posts per page, minimum 1

        for keyword in search_keywords[:3]:  # Limit to 3 keywords
            page = 1
            while page <= max_pages:
                try:
                    params = {
                        "id": gallery_id,
                        "s_type": "search_subject_memo",
                        "s_keyword": keyword,
                        "page": page
                    }

                    response = self.session.get(base_url, params=params, timeout=10)

                    if response.status_code != 200:
                        print(f"   âš ï¸ Error {response.status_code}")
                        break

                    soup = BeautifulSoup(response.text, "html.parser")

                    # Find post links
                    posts = soup.select("tr.ub-content")

                    if not posts:
                        break

                    for post in posts:
                        try:
                            # Get post number
                            num_elem = post.select_one("td.gall_num")
                            if not num_elem:
                                continue

                            post_num = num_elem.get_text(strip=True)
                            if not post_num.isdigit():
                                continue

                            if post_num in posts_processed:
                                continue
                            posts_processed.add(post_num)

                            # Get title
                            title_elem = post.select_one("td.gall_tit a")
                            if not title_elem:
                                continue

                            title = title_elem.get_text(strip=True)
                            post_url = f"https://gall.dcinside.com/mgallery/board/view/?id={gallery_id}&no={post_num}"

                            # Fetch post content
                            time.sleep(DCINSIDE_DELAY)
                            post_response = self.session.get(post_url, timeout=10)

                            if post_response.status_code != 200:
                                continue

                            post_soup = BeautifulSoup(post_response.text, "html.parser")
                            content_elem = post_soup.select_one("div.write_div")

                            if not content_elem:
                                continue

                            content = content_elem.get_text(separator="\n", strip=True)
                            full_text = f"{title}\n{content}"

                            # Extract Korean skill patterns
                            skills = self._extract_korean_skills(full_text)
                            pob_code = self.extract_pob_code(full_text)

                            for skill_pair in skills:
                                pattern = {
                                    "final_skill": skill_pair["final_skill"],
                                    "leveling_skill": skill_pair["leveling_skill"],
                                    "class": self._extract_class(full_text),
                                    "ascendancy": self._extract_ascendancy(full_text),
                                    "transition_point": self._guess_transition_point(full_text),
                                    "source": "dcinside",
                                    "url": post_url,
                                    "pob_code": pob_code
                                }
                                self.patterns.append(pattern)
                                self.stats["dcinside"]["patterns_found"] += 1

                            self.stats["dcinside"]["posts_scanned"] += 1

                        except Exception as e:
                            continue

                    page += 1
                    time.sleep(DCINSIDE_DELAY)

                except Exception as e:
                    print(f"   âŒ Error: {e}")
                    break

        print(f"   âœ… ë””ì‹œì¸ì‚¬ì´ë“œ: {self.stats['dcinside']['posts_scanned']}ê°œ í¬ìŠ¤íŠ¸ ìŠ¤ìº”, "
              f"{self.stats['dcinside']['patterns_found']}ê°œ íŒ¨í„´ ë°œê²¬")

    def _extract_korean_skills(self, text: str) -> list:
        """í•œêµ­ì–´ í…ìŠ¤íŠ¸ì—ì„œ ìŠ¤í‚¬ íŒ¨í„´ ì¶”ì¶œ"""
        found_skills = []

        # Korean skill name mappings
        korean_skills = {
            # Brands
            "ì¢…ë§ì˜ ë‚™ì¸": "Armageddon Brand",
            "ì•„ë§ˆê²Ÿëˆ ë‚™ì¸": "Armageddon Brand",
            "ì†ì£„ì˜ ë‚™ì¸": "Penance Brand",
            "ì†Œì‹¤ì†ë‚™": "Penance Brand of Dissipation",
            "í­í’ ë‚™ì¸": "Storm Brand",
            "ê²¨ìš¸ ì¡°ë¥˜ì˜ ë‚™ì¸": "Wintertide Brand",

            # Spells
            "êµ¬ë¥´ëŠ” ë§ˆê·¸ë§ˆ": "Rolling Magma",
            "ì–¼ìŒ ì°½": "Freezing Pulse",
            "ì „ê¸°ë¶ˆê½ƒ": "Arc",
            "ë¶ˆê½ƒíƒ„": "Fireball",
            "ì „ê´‘ì„í™”": "Spark",
            "ì •ì˜ì˜ í™”ì—¼ í† í…œ": "Holy Flame Totem",
            "ì •í™”ì˜ ë¶ˆê½ƒ": "Righteous Fire",

            # Attacks
            "ê°•ì²  ê°€ë¥´ê¸°": "Splitting Steel",
            "íšŒì „ë² ê¸°": "Cyclone",
            "ì–‘ì†ë² ê¸°": "Cleave",
            "ì§€ë©´ ê°•íƒ€": "Ground Slam",
            "ì§€ì§„": "Earthquake",
            "ìœ ë ¹ ë‚˜ì„ ": "Spectral Helix",
            "ë²ˆê°œ íƒ€ê²©": "Lightning Strike",

            # Minions
            "ë¶„ë…¸í•˜ëŠ” ì˜í˜¼ ì†Œí™˜": "Summon Raging Spirit",
            "í•´ê³¨ ì†Œí™˜": "Summon Skeletons",
            "ì‚¬ë©´": "Absolution",
        }

        # Look for patterns like "ì•¡íŠ¸: X, ë§µ: Y" or "ë ˆë²¨ë§: X -> Y"
        patterns = [
            r"ì•¡íŠ¸[:\s]*([ê°€-í£\s]+)[,\s]*(?:ë§µ|ì´í›„)[:\s]*([ê°€-í£\s]+)",
            r"ë ˆë²¨ë§[:\s]*([ê°€-í£\s]+)[â†’\->]+\s*([ê°€-í£\s]+)",
            r"([ê°€-í£\s]+)ìœ¼?ë¡œ\s*ë ˆë²¨ë§.*?([ê°€-í£\s]+)ìœ¼?ë¡œ\s*ì „í™˜",
        ]

        for pattern in patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                leveling_kr, final_kr = match[0].strip(), match[1].strip()

                # Convert to English
                leveling_en = None
                final_en = None

                for kr, en in korean_skills.items():
                    if kr in leveling_kr:
                        leveling_en = en
                    if kr in final_kr:
                        final_en = en

                if leveling_en and final_en:
                    found_skills.append({
                        "leveling_skill": leveling_en,
                        "final_skill": final_en
                    })

        # Also check for direct mentions
        for kr_leveling, en_leveling in korean_skills.items():
            if kr_leveling in text:
                for kr_final, en_final in korean_skills.items():
                    if kr_final in text and kr_leveling != kr_final:
                        # Check if they appear in leveling -> final context
                        if any(word in text for word in ["ë ˆë²¨ë§", "ì•¡íŠ¸", "ì „í™˜", "ì´í›„"]):
                            found_skills.append({
                                "leveling_skill": en_leveling,
                                "final_skill": en_final
                            })

        # Remove duplicates
        seen = set()
        unique_skills = []
        for skill in found_skills:
            key = (skill["leveling_skill"], skill["final_skill"])
            if key not in seen:
                seen.add(key)
                unique_skills.append(skill)

        return unique_skills

    # ==================== GitHub Crawler ====================

    def crawl_github(self, limit: int = 50):
        """GitHubì—ì„œ POE ë¹Œë“œ ê°€ì´ë“œ ì €ì¥ì†Œ ê²€ìƒ‰"""
        print(f"\nğŸ” GitHub í¬ë¡¤ë§ ì‹œì‘...")

        # Search for POE build repositories
        search_url = "https://api.github.com/search/repositories"

        queries = [
            "path of exile build guide",
            "poe leveling guide",
            "poe build pob",
        ]

        repos_processed = set()

        for query in queries:
            try:
                params = {
                    "q": query,
                    "sort": "updated",
                    "order": "desc",
                    "per_page": min(limit // len(queries), 30)
                }

                response = self.session.get(search_url, params=params, timeout=10)

                if response.status_code == 403:  # Rate limited
                    print(f"   âš ï¸ GitHub rate limited")
                    break

                if response.status_code != 200:
                    continue

                data = response.json()
                repos = data.get("items", [])

                for repo in repos:
                    repo_name = repo.get("full_name")

                    if repo_name in repos_processed:
                        continue
                    repos_processed.add(repo_name)

                    # Get README
                    readme_url = f"https://api.github.com/repos/{repo_name}/readme"
                    time.sleep(GITHUB_DELAY)

                    readme_response = self.session.get(readme_url, timeout=10)

                    if readme_response.status_code != 200:
                        continue

                    readme_data = readme_response.json()
                    content = readme_data.get("content", "")

                    try:
                        readme_text = base64.b64decode(content).decode("utf-8")
                    except:
                        continue

                    # Extract patterns
                    skills = self.extract_skills_from_text(readme_text)
                    pob_code = self.extract_pob_code(readme_text)

                    for skill_pair in skills:
                        pattern = {
                            "final_skill": skill_pair["final_skill"],
                            "leveling_skill": skill_pair["leveling_skill"],
                            "class": self._extract_class(readme_text),
                            "ascendancy": self._extract_ascendancy(readme_text),
                            "transition_point": self._guess_transition_point(readme_text),
                            "source": "github",
                            "url": repo.get("html_url"),
                            "pob_code": pob_code
                        }
                        self.patterns.append(pattern)
                        self.stats["github"]["patterns_found"] += 1

                    self.stats["github"]["repos_scanned"] += 1

                time.sleep(GITHUB_DELAY)

            except Exception as e:
                print(f"   âŒ Error: {e}")
                continue

        print(f"   âœ… GitHub: {self.stats['github']['repos_scanned']}ê°œ ì €ì¥ì†Œ ìŠ¤ìº”, "
              f"{self.stats['github']['patterns_found']}ê°œ íŒ¨í„´ ë°œê²¬")

    # ==================== Helper Methods ====================

    def _extract_class(self, text: str) -> Optional[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ í´ë˜ìŠ¤ ì¶”ì¶œ"""
        classes = [
            "Marauder", "Templar", "Witch", "Duelist", "Ranger", "Shadow", "Scion"
        ]
        text_lower = text.lower()
        for cls in classes:
            if cls.lower() in text_lower:
                return cls
        return None

    def _extract_ascendancy(self, text: str) -> Optional[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ ì „ì§ ì¶”ì¶œ"""
        ascendancies = [
            # Marauder
            "Juggernaut", "Berserker", "Chieftain",
            # Templar
            "Inquisitor", "Hierophant", "Guardian",
            # Witch
            "Necromancer", "Elementalist", "Occultist",
            # Duelist
            "Slayer", "Gladiator", "Champion",
            # Ranger
            "Deadeye", "Raider", "Pathfinder",
            # Shadow
            "Assassin", "Trickster", "Saboteur",
            # Scion
            "Ascendant"
        ]
        text_lower = text.lower()
        for asc in ascendancies:
            if asc.lower() in text_lower:
                return asc
        return None

    def _guess_transition_point(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ì—ì„œ ì „í™˜ ì‹œì  ì¶”ì¸¡"""
        text_lower = text.lower()

        if any(word in text_lower for word in ["4th ascendancy", "uber lab", "4ì°¨ ì „ì§"]):
            return "4th_ascendancy"
        elif any(word in text_lower for word in ["maps", "mapping", "ë§µ", "ë§µí•‘"]):
            return "maps_entry"
        elif any(word in text_lower for word in ["act 10", "act10", "10ë§‰"]):
            return "act_complete"
        elif any(word in text_lower for word in ["level 70", "level 80", "70ë ˆë²¨", "80ë ˆë²¨"]):
            return "specific_level"

        return "maps_entry"  # Default

    def run_all(self, reddit_limit: int = 100, dcinside_limit: int = 50, github_limit: int = 30):
        """ëª¨ë“  í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
        print("=" * 50)
        print("ë¹Œë“œ ì „í™˜ íŒ¨í„´ ìˆ˜ì§‘ê¸° ì‹œì‘")
        print("=" * 50)

        # Run crawlers
        self.crawl_reddit(limit=reddit_limit)
        self.crawl_dcinside(limit=dcinside_limit)
        self.crawl_github(limit=github_limit)

        # Save results
        self.save_patterns()

        # Print summary
        print("\n" + "=" * 50)
        print("ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½")
        print("=" * 50)
        print(f"Reddit: {self.stats['reddit']['posts_scanned']}ê°œ ìŠ¤ìº” â†’ "
              f"{self.stats['reddit']['patterns_found']}ê°œ íŒ¨í„´")
        print(f"ë””ì‹œì¸ì‚¬ì´ë“œ: {self.stats['dcinside']['posts_scanned']}ê°œ ìŠ¤ìº” â†’ "
              f"{self.stats['dcinside']['patterns_found']}ê°œ íŒ¨í„´")
        print(f"GitHub: {self.stats['github']['repos_scanned']}ê°œ ìŠ¤ìº” â†’ "
              f"{self.stats['github']['patterns_found']}ê°œ íŒ¨í„´")
        print(f"\nì´ {len(self.patterns)}ê°œ íŒ¨í„´ ìˆ˜ì§‘ ì™„ë£Œ")

        return self.patterns


if __name__ == "__main__":
    crawler = BuildPatternCrawler()
    patterns = crawler.run_all(
        reddit_limit=100,
        dcinside_limit=50,
        github_limit=30
    )
